# Topic 6 Exercises: Selecting Model Terms

# Theory

6.8.1

(a) Best subset has smallest training RSS since it finds the absolute best model for that data, and therefore has the smallest residuals.
(b) It is hard to know for sure which will have the smallest test RSS but it is most likely that it is best subset again, since it considers all possible models.
(c) i. True
    ii. True
    iii. False
    iv. False
    v. False

6.8.2.

(a) The lasso, relative to least squares, is:
    iii. Less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance.
(b) The ridge regression, relative to least squares, is:
    iii. Same as lasso (above)
(c) Non-linear methods, relative to least squares, are:
    ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

# Applied

6.8.9

(a)
```{r}
model_formula <- Apps ~.
library(ISLR)
data(College)
names(College)
College$Apps <- log10(College$Apps)
training_cases <- sample(1:nrow(College), size = nrow(College)/2)
Training <- College[training_cases,]
Testing <- College[ - training_cases,]
```

(b)
```{r}
mod_b <- lm(model_formula, data = Training)
preds <- predict(mod_b, newdata = Testing)
mean((Testing$Apps - preds)^2)
```
Test RSS = 1,088,625

(c)
```{r}
library(glmnet)

Training_MM <- model.matrix(model_formula, data = Training)
mod_c <- cv.glmnet(Training_MM , Training$Apps, alpha = 0)
mod_c$lambda.min

Testing_MM <- model.matrix(model_formula, data = Testing)
preds <- predict(mod_c, newx = Testing_MM, s = mod_c$lambda.min)
mean((Testing$Apps - preds)^2)
```


(d)
```{r}
library(glmnet)
Training_MM <- model.matrix(model_formula, data = Training)
mod_d <- cv.glmnet(Training_MM , Training$Apps, alpha = 1)
mod_d$lambda.min
Testing_MM <- model.matrix(model_formula, data = Testing)
preds <- predict(mod_d, newx = Testing_MM, s = mod_d$lambda.min)
mean((Testing$Apps - preds)^2)
```


(e)
```{r}
library(pls)
mod_e <- pcr(model_formula, data = Training, validation = "CV")
validationplot(mod_e, val.type = "MSEP")
preds <- predict(mod_e,  newdata = Testing, ncomp = 4)
mean((Testing$Apps - preds)^2)
```
(f)
```{r}
library(pls)
mod_f <- plsr(model_formula, data = Training, validation = "CV")
validationplot(mod_f, val.type = "MSEP")
preds <- predict(mod_f,  newdata = Testing, ncomp = 4)
mean((Testing$Apps - preds)^2)
```

(g)
The average square prediction error is ~ 140,000 for linear regression. The result depends highly on what is selected for in the training set. Additionally, there is a significant outlier in College Apps. 